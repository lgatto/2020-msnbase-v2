\documentclass[journal=jacsat,manuscript=article]{achemso}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother


\usepackage{subfig}
\usepackage{graphicx}
\usepackage{alltt}

\usepackage{chemformula} % Formula subscripts using \ch{}
\usepackage[T1]{fontenc} % Use modern font encodings

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% If issues arise when submitting your manuscript, you may want to
%% un-comment the next line.  This provides information on the
%% version of every file you have used.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\listfiles

\newcommand*\mycommand[1]{\texttt{\emph{#1}}}

\author{Laurent Gatto}
\email{laurent.gatto@uclouvain.be}
\affiliation[UCLouvain]{Computational Biology Unit, de Duve Institute, Universit\'e catholique de Louvain, Brussels, Belgium}
\author{Sebastian Gibb}
\affiliation[University of Greifswald]{Department of Anaesthesiology and Intensive Care of the University Medicine Greifswald, Germany}
\author{Johannes Rainer}
\affiliation[Eurac Research]{Institute for Biomedicine, Eurac Research, Affiliated Institute of the University of L\"ubeck, Bolzano, Italy}


\title[MSnbase version 2]
  {\texttt{MSnbase}, efficient and elegant R-based processing and
    visualisation of raw mass spectrometry data}

\abbreviations{}
\keywords{Bioconductor, mass spectrometry, software, metabolomics, proteomics, visualisation, reproducible research} %% up to 10 keywords
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

%% \begin{tocentry}
%% See achemso-demo.tex
%% \end{tocentry}

\begin{abstract} %% 200 words max
  We present version 2 of the \texttt{MSnbase} R/Bioconductor
  package. \texttt{MSnbase} provides infrastructure for the
  manipulation, processing and visualisation of mass spectrometry
  data. We focus on the new \textit{on-disk} infrastructure, that
  allows the handling of large raw mass spectrometry experiment on
  commodity hardware and illustrate how the package is used for
  elegant data processing, method development and visualisation.
\end{abstract}

<<setup, include = FALSE, message = FALSE>>=
knitr::opts_chunk$set(echo = FALSE, cache = FALSE)
library("tidyverse")
library("patchwork")
@

\section{Introduction}

Mass spectrometry is a powerful technology to assay chemical and
biological samples. It is used in routine with well characterised
protocol, as well a development platform, to improve on existing
methods and devise new ones. The complexity and diversity of mass
spectrometry yield data that is itself complex and often times of
considerable size, that require non trivial processing before
producing interpretable results. This is particularly relevant, and
can constitute a significant challenge for method development: in
addition to the development of sample processing and mass spectrometry
methods, there is a need to process, analyse, interpret and assess
these new data to demonstrate the improvement in the technical,
analytical and computational workflows.

Practitioners have a diverse catalogue of software tools to explore,
process and interpret mass spectrometry data at their disposal. These
range from low level software libraries that are aimed at programmers
to enable development of new applications, to more user-oriented
applications with graphical user interfaces which provide a more
limited set of functionalities to address a defined scope. Examples of
software libraries include Java-based jmzML~\cite{Cote:2010} or
C/C++-based ProteoWizard~\cite{Chambers:2012}. ProteomeDiscoverer
(Thermo Scientific), MaxQuant~\cite{Cox:2008} and
PeptideShaker~\cite{Vaudel:2015} are among the most widely used
user-centric applications.

In this software note, we present version 2 of the \texttt{MSnbase}
\cite{Gatto:2012} software, available from the
Bioconductor~\cite{Huber:2015} project. \texttt{MSnbase}, like other
software such as Python-based {pyOpenMS}~\cite{Rost:2014},
spectrum\_utils~\cite{Bittremieux:2020} or
Pyteomics~\cite{Goloborodko:2013}, offers a platform that lies between
low level libraries and end-user software. \texttt{MSnbase} provides a
flexible R~\cite{R} command line environment for metabolomics and
proteomics mass spectrometry-based applications. It allows a detailed
step-by-step processing, analysis and exploration of the data and
development of novel computational mass spectrometry
methods~\cite{Stanstrup:2019}.

\section{Software functionality}

In \texttt{MSnbase}, mass spectrometry experiments are handled as
\texttt{MSnExp} objects. While the implementation is more complex, it
is useful to schematise a raw data experiment as being composed of raw
data, i.e. a collection of individual spectra, as well as
spectra-level metadata. Each spectrum is composed of m/z values and
associated intensities. The metadata are represented by a table with
variables along the columns and each row associated to a
spectrum. Among the metadata available for each spectrum, there are MS
level, acquisition number, retention time, precursor m/z and intensity
(for MS level 2 and above), and many more. \texttt{MSnbase} provides a
rich interface to manipulate such objects. The code chunk below
illustrates such an object as displayed in the R console and an
enumeration of the metadata fields.

<<msnexp, echo = TRUE, eval = FALSE>>=
> show(ms)
MSn experiment data ("OnDiskMSnExp")
Object size in memory: 0.54 Mb
- - - Spectra data - - -
 MS level(s): 1 2 3 
 Number of spectra: 994 
 MSn retention times: 45:27 - 47:6 minutes
- - - Processing information - - -
Data loaded [Sun Apr 26 15:40:58 2020] 
 MSnbase version: 2.13.6 
- - - Meta data  - - -
phenoData
  rowNames: MS3TMT11.mzML
  varLabels: sampleNames
  varMetadata: labelDescription
Loaded from:
  MS3TMT11.mzML 
protocolData: none
featureData
  featureNames: F1.S001 F1.S002 ... F1.S994 (994 total)
  fvarLabels: fileIdx spIdx ... spectrum (35 total)
  fvarMetadata: labelDescription
experimentData: use 'experimentData(object)'
> fvarLabels(ms)
 [1] "fileIdx"                    "spIdx"
 [3] "smoothed"                   "seqNum"
 [5] "acquisitionNum"             "msLevel"
 [7] "polarity"                   "originalPeaksCount"
 [9] "totIonCurrent"              "retentionTime"
[11] "basePeakMZ"                 "basePeakIntensity"
[13] "collisionEnergy"            "ionisationEnergy"
[15] "lowMZ"                      "highMZ"
[17] "precursorScanNum"           "precursorMZ"
[19] "precursorCharge"            "precursorIntensity"
[21] "mergedScan"                 "mergedResultScanNum"
[23] "mergedResultStartScanNum"   "mergedResultEndScanNum"
[25] "injectionTime"              "filterString"
[27] "spectrumId"                 "centroided"
[29] "ionMobilityDriftTime"       "isolationWindowTargetMZ"
[31] "isolationWindowLowerOffset" "isolationWindowUpperOffset"
[33] "scanWindowLowerLimit"       "scanWindowUpperLimit"
[35] "spectrum"
@

In the following sections, we describe \texttt{MSnbase}'s ability to efficiently
manage very large mass spectrometry data and experiments and how it can be used
for data processing and visualisation. We will also illustrate how it makes use
of the forward-pipe operator (\texttt{\%>\%}) defined in the \texttt{magrittr}
package. This operator has proved useful to develop non-trivial analyses by
combining individual functions into easily readable pipelines.

\subsection{On-disk backend}

The main feature in version 2 of the \texttt{MSnbase} package was the
addition of different backends for raw data storage, namely
\textit{in-memory} and \textit{on-disk}. The following code chunk
demonstrates how to create two \texttt{MSnExp} objects storing data
either in-memory or on-disk. 

<<readMSData, eval = FALSE, echo = TRUE>>=
library("MSnbase")
raw_mem <- readMSData("file.mzML", mode = "inMemory")
raw_dsk <- readMSData("file.mzML", mode = "onDisk")
@

<<time_sz>>=
load("bench_time_sz.rda")
time <- sapply(time_sz, function(x) x[, "time"])
sz <- sapply(time_sz, function(x) x[, "sz"])
colnames(sz) <- colnames(time) <- c(1, 5, 10)
rownames(sz) <- rownames(time) <- c("in-memory", "on-disk")
sz <- round(sz/(1014^2), 2)

p_read <- 
    data.frame(time) %>% 
    rownames_to_column() %>% 
    pivot_longer(names_to = "files", 
                 values_to = "time", 
                 -rowname) %>% 
    rename(backend = rowname) %>% 
    mutate(files = as.numeric(sub("X", "", files))) %>% 
    ggplot(aes(x = files, y = time, colour = backend)) + 
    geom_point() + 
    geom_line() + 
    scale_x_continuous(breaks=c(1, 5, 10)) + 
    ylab("Reading time [s]") + 
    xlab("Number of files") +
    theme(legend.position = "none")

p_sz <- 
    data.frame(sz) %>% 
    rownames_to_column() %>% 
    pivot_longer(names_to = "files", 
                 values_to = "size",
                 -rowname) %>% 
    rename(backend = rowname) %>% 
    mutate(files = as.numeric(sub("X", "", files))) %>% 
    ggplot(aes(x = files, y = size, colour = backend)) + 
    geom_point() + 
    geom_line() + 
    scale_x_continuous(breaks=c(1, 5, 10)) + 
    ylab("Object size (MB)") +
    xlab("Number of files") +
    theme(legend.position = "none")    
@

<<filt>>=
load("bench_t_filt.rda")
p_filt <- 
    tibble(time = microbenchmark:::convert_to_unit(t_filt$time, "ms"),
           expr = as.character(t_filt$expr)) %>%    
    mutate(mode = if_else(grepl("mem", expr), "in-memory", "on-disk")) %>%
    ggplot(aes(x = mode, y = time, fill = mode)) + 
    ggplot2::geom_violin() + 
    ggplot2::scale_y_log10() +
    ylab("Filtering time [millisec]") + 
    xlab("Backend") +    
    theme(legend.position = "none")

@

<<access>>=
load("bench_t_access.rda")
p_access <- 
    tibble(time = microbenchmark:::convert_to_unit(t_access$time, "ms"),
           expr = as.character(t_access$expr)) %>%
    mutate(n = sub("^access_.+_", "", expr)) %>%
    mutate(n = sub("all", 6103, n)) %>%
    mutate(mode = if_else(grepl("mem", expr), "in-memory", "on-disk")) %>%
    ggplot(aes(x = n, y = time, fill = mode)) + 
    ggplot2::geom_violin() + 
    ggplot2::scale_y_log10() +
    ylab("Raw data access [millisec]") +
    xlab("Number of spectra (out of 6103)") +
    facet_wrap(~ mode) +
    theme(legend.position = "none")
@

\begin{figure}[p]
  \centering
<<plot_bench>>=
(p_read + p_sz + p_filt) / p_access + 
    plot_layout(heights = c(1, 0.75)) + 
    plot_annotation(tag_levels = 'a') &
    theme(axis.title = element_text(size = 10))
@
\caption{(a) Reading time (in seconds) and (b) data size in memory (in
  MB) to read/store 1, 5 and 10 files containing 1431 MS1 (on-disk
  only) and 6103 MS2 (on-disk and in-memory) spectra. (c) Filtering
  benchmark assessed over 10 interactions on in-memory and on-disk
  data containing 6103 MS2 spectra.  (d) Access time to spectra for
  the in-memory (left) and on-disk (right) backends for 1, 10, 100
  1000, 5000 and all 6103 spectra. On-disk backend: blue. In-memory
  backend: red. Benchmarks were performed on a Dell XPS laptop with an
  Intel i5-8250U processor 1.60 GHz (4 cores, 8 threads) and 7.5 GB RAM
  running Ubuntu 18.04.4 LTS 64-bit. }
\label{fig:bench}
\end{figure}

Both modes rely on the \texttt{mzR}~\cite{Chambers:2012} package to
access the spectra (using the \texttt{mzR::peaks()} function) and the
metadata (using the \texttt{mzR::header()} function) in the data
files. The former is the legacy storage mode, implemented in the first
version of the package, that loads all the raw data and the metadata
into memory upon creation of the in-memory \texttt{MSnExp}
object. This solution doesn't scale for modern large dataset, and was
complemented by the on-disk backend. The on-disk backend only loads
the metadata into memory when the on-disk \texttt{MSnExp} is created
and accesses the spectra data (i.e. m/z and intensity values) in the
original files on disk only when needed (see below and Figure
\ref{fig:bench} (d)), such as for example for plotting. There are two
direct benefits using the on-disk backend, namely faster reading and
reduced memory footprint. Figure \ref{fig:bench} shows 5-fold faster
reading times (a) and over a 10-fold reduction in memory usage (b).

The on-disk backend also offers efficient data manipulation by way of
\textit{lazy processing} in the object itself. Operations on the raw
data are stored in a processing queue and only effectively applied
when the user accesses m/z or intensity values. As an example, the
following short analysis pipeline, that can equally be applied to
in-memory or on-disk data, retains MS2 spectra acquired between 1000
and 3000 seconds, extracts the m/z range corresponding to the TMT
6-plex range and focuses on the MS2 spectra with a precursor intensity
greater than $11 \times 10^6$ (the median precursor intensity).

<<filter, eval = FALSE, echo = TRUE>>=
ms <- ms %>%
    filterRt(c(1000, 3000)) %>%
    filterMz(120, 135)
ms[precursorIntensity(ms) > 11e6, ]
@

As shown on Figure~\ref{fig:bench} (c), this lazy mechanism is
significantly faster than its application on in-memory data. The
advantageous reading and execution times and memory footprint of the
on-disk backend are possible by retrieving only spectra data from the selected
subset hence avoiding access to the full raw
data. Once access to the spectra m/z and intensity values become
mandatory (for example for plotting), then the in-memory backend
becomes more efficient, as illustrated on Figure~\ref{fig:bench}
(d). This gain is maximal when the whole dataset is accessed
(i.e. all spectra are already in memory) and negligible when only subsets of the
data are requested.


\subsection{Prototyping}

The \texttt{MSnExp} data structure and its interface constitute a
efficient prototyping environment for computational method
development. We illustrate this by demonstrating how to implement the
BoxCar\cite{Meier:2018} acquisition method. In a nutshell, BoxCar
acquisition aims at improving the detection of intact precursor ions
by distributing the charge capacity over multiple narrow m/z segments
and thus limiting the proportion of highly abundant precursors in each
segment. A full scan is reconstructed by combining the respective
adjacent segments of the BoxCar acquisitions. The
\texttt{MSnbaseBoxCar} package\cite{MSnbaseBoxCar} is a small package
that demonstrates this. The simple method is composed of three steps
is described below and illustrated with code from
\texttt{MSnbaseBoxCar} in the following code chunk.

\begin{enumerate}

\item Identify and filter the groups of spectra that represent
  adjacent BoxCar acquisitions (Figure~\ref{fig:bc}~(b)). This can be
  done using the \textit{filterString} metadata variable that
  identifies BoxCar spectra by their adjacent m/z segments with the
  \texttt{bc\_groups()} function and filtering relevant spectra with
  the \texttt{filterBoxCar()}.
  
\item Remove any signal outside the BoxCar segments using the
  \texttt{bc\_zero\_out\_box()} function from \texttt{MSnbaseBoxCar}
  (Figures~\ref{fig:bc}~(c) and (d)).


\item Using the \texttt{combineSpectra} function from the
  \texttt{MSnbase}, combine the cleaned BoxCar spectra into a new,
  full spectrum (Figure~\ref{fig:bc}~(e)).

\end{enumerate}

<<bc1, echo=TRUE, eval=FALSE>>=
bc <- readMSData("boxcar.mzML", mode = "onDisk") %>%
      bc_groups() %>%       ## identify BoxCar groups (bc_groups)
      filterBoxCar() %>%    ## keep only BoxCar spectra
      bc_zero_out_box() %>% ## remove signal outside of BoxCar segments
      combineSpectra(fcol = "bc_groups", ## reconstruct full spectrum
                     method = boxcarCombine)
@  



\begin{figure}[p]
  \centering
<<plot_boxcar, message=FALSE, warning=FALSE, fig.height=8>>=
load("boxcar.rda")
p1 + p2 + p3 + p4 + 
    plot_layout(ncol = 1, 
                heights = c(0.7, 1.2, 0.7, 0.7)) + 
    plot_annotation(tag_levels = 'a') &
    theme(axis.title = element_text(size = 10),
          axis.text = element_text(size = 6),
          plot.margin = margin(0, 0, 0, 0, "mm"))
@
\caption{BoxCar processing with \texttt{MSnbase}. (a) Standard full
  scan with (b) three corresponding BoxCar scans showing the adjacent
  segments. Figure (c) shows the overlapping intact BoxCar segments
  and (d) the same segments after cleaning, i.e. where peaks outside
  of the segments were removed. The reconstructed full scan is shown
  on panel (e).}
\label{fig:bc}
\end{figure}

All the functions for the processing of BoxCar spectra and segments in
\texttt{MSnbaseBoxCar} were developed using existing functionality
implemented in \texttt{MSnbase}, illustrating the flexibility and
adaptability of the \texttt{MSnbase} package for computational mass
spectrometry method development.


\subsection{Visualisation}

The R environment is well known for the quality of its visualisation
capacity. This also holds true for mass
spectrometry\cite{Gatto:2015,protViz,Pviz,Bemis:2015}. Here, we
conclude the overview of version 2 of the \texttt{MSnbase} package by
highlighting the flexibility of the software to visualise and assess
the efficiency of raw data processing. Figure \ref{fig:cent} compares
the raw MS profile data for the serine and the same data after
smoothing, centroiding and m/z refinement, as illustrated in the code
chunk below. Detailed execution and description of these operations
can be found in the \emph{MSnbase: centroiding of profile-mode MS
  data} \texttt{MSnbase} vignette.

<<serine_processing, echo=TRUE, eval=FALSE>>=
serine_mz <- 106.049871
serine_proc <- ms %>%
    smooth(method = "SavitzkyGolay", halfWindowSize = 4L) %>% 
    pickPeaks(refineMz = "descendPeak") %>%
    filterMz(c(serine_mz - 0.01, serine_mz + 0.01)) %>%
    filterRt(c(175, 187))
@

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./figure/centroiding.pdf}
  \caption{Visualisation of data smoothing and m/z refinement using
    \texttt{MSnbase}. (a) Raw MS profile data for serine. Upper panel
    shows the base peak chromatogram (BPC), lower panel the individual
    signals in the retention time -- m/z space. The horizontal dashed
    red line indicates the theoretical m/z of the [M+H]+ adduct of
    serine. (b) Smoothed and centroided data for serine with m/z
    refinement. The horizontal red dashed line indicates the
    theoretical m/z for the [M+H]+ ion and the vertical red dotted
    line the position of the maximum signal. }
  \label{fig:cent}
\end{figure}


\section{Discussion}

We have presented here some some important functionality of
\texttt{MSnbase} version 2. The new on-disk infrastructure enables
large scale data analyses~\cite{Nothias:2020}, either using
\texttt{MSnbase} directly or through packages that rely on it, such as
\texttt{xcms}. We have also illustrated how \texttt{MSnbase} can be
used for standard data analysis and visualisation, and how it can be
used for method development and prototyping.

The first public commit to the \texttt{MSnbase} GitHub repository was
in October 2010. Since then, the package benefited from 12
contributors\cite{contribs} that added various features, some
particularly significant ones such as the on-disk backend described
herein. According to \texttt{MSnbase}'s Bioconductor page, there are
36 Bioconductor packages that depend, import or suggest it. Among
these are \texttt{pRoloc}~\cite{Gatto:2014a} to analyse mass
spectrometry-based spatial proteomics data,
\texttt{msmsTests}~\cite{msmsTests}, \texttt{DEP}~\cite{Zhang:2018},
\texttt{DAPAR} and \texttt{ProStaR}~\cite{Wieczorek:2017} for the
statistical analysis for quantitative proteomics data,
\texttt{RMassBank}~\cite{Stravs:2013} to process metabolomics tandem
MS files and build MassBank records,
\texttt{MSstatsQC}~\cite{Dogu:2017} for longitudinal system
suitability monitoring and quality control of targeted proteomic
experiments and the widely used \texttt{xcms}~\cite{Smith:2006} package
for the processing and analysis of metabolomics data. \texttt{MSnbase}
is also used in non-R/Bioconductor software, such as for example
IsoProt~\cite{Griss:2019}, that provides a reproducible workflow for
iTRAQ/TMT experiments. \texttt{MSnbase} currently ranks 101 out of
1823 packages based on the monthly downloads from unique IP addresses,
tallying over 1000 downloads from unique IP addresses each months.

As is custom with Bioconductor packages, \texttt{MSnbase} comes with
ample documentation. Every user-accessible function is documented in a
dedicated manual page. In addition, the package offers 5 vignettes,
including one aimed at developers. The package is checked nightly on
the Bioconductor servers: it implements unit tests covering 72\% of
the code base and, through its vignettes, also provides integration
testing. Questions from users and developers are answered on the
Bioconductor support forum as well as on the package GitHub page. The
package provides several sample and benchmarking datasets, and relies
on other dedicated \textit{experiment packages} such as
\texttt{msdata}~\cite{msdata} for raw data or
\texttt{pRolocdata}~\cite{Gatto:2014a} for quantitative
data. \texttt{MSnbase} is available on Windows, Mac OS and Linux under
the open source Artistic 2.0 license and easily installable using
standard installation procedures.


<<version>>=
v <- packageVersion("MSnbase")
@

The version of \texttt{MSnbase} used in this manuscript is
\Sexpr{v}. The main features presented here were available since
version 2.0. The code to reproduce the analyses and figures in this
article is available at
\url{https://github.com/lgatto/2020-msnbase-v2/}.


\begin{acknowledgement}

The authors thank the various contributors and users who have provided
constructive input and feedback that have helped, over the years, the
improvement of the package. The authors declare no conflict of
interest.

\end{acknowledgement}

%% \begin{suppinfo} %% Supporting Information
%%
%% A listing of the contents of each file supplied as Supporting Information
%% should be included. For instructions on what should be included in the
%% Supporting Information as well as how to prepare this material for
%% publications, refer to the journal's Instructions for Authors.
%%
%% The following files are available free of charge.
%% \begin{itemize}
%%   \item Filename: brief description
%%   \item Filename: brief description
%% \end{itemize}
%% \end{suppinfo}


\bibliography{refs}

\end{document}
