\documentclass[journal=jacsat,manuscript=article]{achemso}

\usepackage[]{graphicx}
\usepackage[]{color}

\usepackage[]{xcolor}
\usepackage[normalem]{ulem}

%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother


\usepackage{subfig}
\usepackage{graphicx}
\usepackage{alltt}

\usepackage{chemformula} % Formula subscripts using \ch{}
\usepackage[T1]{fontenc} % Use modern font encodings

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% If issues arise when submitting your manuscript, you may want to
%% un-comment the next line.  This provides information on the
%% version of every file you have used.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\listfiles

\newcommand*\mycommand[1]{\texttt{\emph{#1}}}

\author{Laurent Gatto}
\email{laurent.gatto@uclouvain.be}
\affiliation[UCLouvain]{Computational Biology Unit, de Duve Institute, Universit\'e catholique de Louvain, Brussels, Belgium}
\author{Sebastian Gibb}
\affiliation[University of Greifswald]{Department of Anaesthesiology and Intensive Care of the University Medicine Greifswald, Germany}
\author{Johannes Rainer}
\affiliation[Eurac Research]{Institute for Biomedicine, Eurac Research, Affiliated Institute of the University of L\"ubeck, Bolzano, Italy}


\title[MSnbase version 2]
  {\texttt{MSnbase}, efficient and elegant R-based processing and
    visualisation of raw mass spectrometry data}

\abbreviations{}
\keywords{Bioconductor, mass spectrometry, software, metabolomics, proteomics, visualisation, reproducible research} %% up to 10 keywords
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

%% \begin{tocentry}
%% See achemso-demo.tex
%% \end{tocentry}

\begin{abstract} %% 200 words max
  We present version 2 of the \texttt{MSnbase} R/Bioconductor
  package. \texttt{MSnbase} provides infrastructure for the
  manipulation, processing and visualisation of mass spectrometry
  data. We focus on the new \textit{on-disk} infrastructure, that
  allows the handling of large raw mass spectrometry experiment on
  commodity hardware and illustrate how the package is used for
  elegant data processing, method development, and visualisation.
\end{abstract}

<<setup, include = FALSE, message = FALSE>>=
knitr::opts_chunk$set(echo = FALSE, cache = FALSE)
library("tidyverse")
library("patchwork")
@

\section{Introduction}

Mass spectrometry is a powerful technology to assay chemical and
biological samples. It is used in routine applications with well
characterised \textcolor{red}{protocols} \textcolor{red}{such as
  diagnostics}, as well as a development platform,
\textcolor{red}{with the aim} to improve on existing \sout{methods}
\textcolor{red}{protocols} and devise new ones. The complexity and
diversity of mass spectrometry yield data that is itself complex and
often times of considerable size, that require non trivial processing
before producing interpretable results. \sout{This is particularly
  relevant, and can} \textcolor{red}{The complexity and size of these
  data} constitute a significant challenge for \sout{method}
\textcolor{red}{protocol} development: in addition to the development
of sample processing and mass spectrometry methods
\textcolor{red}{that yield the raw data}, \textcolor{red}{it is
  essential} \sout{there is a need} to process, analyse, interpret and
assess these new data to demonstrate the improvement in the technical,
analytical and computational workflows.

Practitioners have a diverse catalogue of software tools to explore,
process and interpret mass spectrometry data at their disposal. These
range from low level software libraries that are aimed at programmers
to enable development of new applications, to more user-oriented
applications with graphical user interfaces which provide a more
limited set of functionalities to address a defined scope. Examples of
software libraries include Java-based jmzML~\cite{Cote:2010} or
C/C++-based ProteoWizard~\cite{Chambers:2012}.
\sout{ProteomeDiscoverer (Thermo Scientific)}
\textcolor{red}{Thermo Scientific Proteome Discoverer (Thermo Fisher Scientific)},
MaxQuant~\cite{Cox:2008} and PeptideShaker~\cite{Vaudel:2015} are among the
most widely used user-centric applications.

In this software note, we present version 2 of the
\texttt{MSnbase}~\cite{Gatto:2012} software, available from the
Bioconductor~\cite{Huber:2015} project. The package, like other
software such as Python-based {pyOpenMS}~\cite{Rost:2014},
spectrum\_utils~\cite{Bittremieux:2020} or
Pyteomics~\cite{Goloborodko:2013}, offers a platform that lies between
low level libraries and end-user software. \texttt{MSnbase} provides a
flexible R~\cite{R} command-line environment for metabolomics and
proteomics mass spectrometry-based applications. It lays out a sound
infrastructure to work with raw mass spectrometry \textcolor{red}{data
  from MS files in mzML, mzXML, mzData or ANDI-MS/netCDF format as
  well as} quantitative and proteomics identification data. The
package enables manipulation (for example subsetting, filtering, or
accessing specific parts thereof), detailed step-by-step processing
(for example smoothing and centroiding of \sout{raw}
\textcolor{red}{profile-mode MS} data, or normalisation and imputation
of quantitative data), analysis and visualisation of these data and
the development of novel computational mass spectrometry
methods~\cite{Stanstrup:2019}. \textcolor{red}{Extensive documentation
  and use cases are provided in \textit{package
    vignettes}~\cite{MSnbaseVignettes} and
  workflows~\cite{xcmsWorkflow}.} Here, we focus on the new
developments pertaining to raw mass spectrometry data handling and
processing.

\section{Infrastructure for raw data}

In \texttt{MSnbase}, mass spectrometry experiments are handled as
\texttt{MSnExp} objects. While the implementation is more complex, it
is useful to schematise a raw data experiment as being composed of raw
data, i.e. a collection of individual spectra, as well as
spectra-level metadata (Figure \ref{fig:raw}). Each spectrum is
composed of m/z values and associated intensities. The metadata are
represented by a \textcolor{red}{single} table with variables along
the columns and each row associated to a spectrum. Among the metadata
available for each spectrum, there are MS level, acquisition number,
retention time, precursor m/z and intensity (for MS level 2 and
above), and many more. \texttt{MSnbase} \textcolor{red}{relies on the
  \texttt{mzR} package~\cite{Chambers:2012} to import raw mass
  spectrometry data from one of the many community-maintained open
  standards formats (mzML, mzXML, mzData or ANDI-MS/netCDF) and}
provides a rich interface to manipulate such objects. The code chunk
below illustrates such an object as displayed in the R console and an
enumeration of the metadata fields.

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{./figure/raw.png}
  \caption{Schematic representation of what is referred to by
    \textit{raw data}: a collection of mass spectra and a table
    containing spectrum-level annotations along the lines. Raw data is
    contained in one of the many community-maintained open standards
    formats (mzML, mzXML, mzData or ANDI-MS/netCDF). }
  \label{fig:raw}
\end{figure}


<<msnexp, echo = TRUE, eval = FALSE>>=
> show(ms)
MSn experiment data ("OnDiskMSnExp")
Object size in memory: 0.54 Mb
- - - Spectra data - - -
 MS level(s): 1 2 3
 Number of spectra: 994
 MSn retention times: 45:27 - 47:6 minutes
- - - Processing information - - -
Data loaded [Sun Apr 26 15:40:58 2020]
 MSnbase version: 2.13.6
- - - Meta data  - - -
phenoData
  rowNames: MS3TMT11.mzML
  varLabels: sampleNames
  varMetadata: labelDescription
Loaded from:
  MS3TMT11.mzML
protocolData: none
featureData
  featureNames: F1.S001 F1.S002 ... F1.S994 (994 total)
  fvarLabels: fileIdx spIdx ... spectrum (35 total)
  fvarMetadata: labelDescription
experimentData: use 'experimentData(object)'
> fvarLabels(ms)
 [1] "fileIdx"                    "spIdx"
 [3] "smoothed"                   "seqNum"
 [5] "acquisitionNum"             "msLevel"
 [7] "polarity"                   "originalPeaksCount"
 [9] "totIonCurrent"              "retentionTime"
[11] "basePeakMZ"                 "basePeakIntensity"
[13] "collisionEnergy"            "ionisationEnergy"
[15] "lowMZ"                      "highMZ"
[17] "precursorScanNum"           "precursorMZ"
[19] "precursorCharge"            "precursorIntensity"
[21] "mergedScan"                 "mergedResultScanNum"
[23] "mergedResultStartScanNum"   "mergedResultEndScanNum"
[25] "injectionTime"              "filterString"
[27] "spectrumId"                 "centroided"
[29] "ionMobilityDriftTime"       "isolationWindowTargetMZ"
[31] "isolationWindowLowerOffset" "isolationWindowUpperOffset"
[33] "scanWindowLowerLimit"       "scanWindowUpperLimit"
[35] "spectrum"
@

In the following sections, we describe \sout{\texttt{MSnbase}'s
  ability to efficiently handle very large mass spectrometry data and
  experiments and how it} \textcolor{red}{how \texttt{MSnbase}} can be
used for data processing and visualisation. \textcolor{red}{An example
  of its ability to efficiently handle also very large mass
  spectrometry experiments (in this case with 5,773,464 spectra in
  1,182 mzXML files) is provided as supplementary information.}  We
will also illustrate how it makes use of the forward-pipe operator
(\texttt{\%>\%}) defined in the \texttt{magrittr} package. This
operator has proved useful to develop non-trivial analyses by
combining individual functions into easily readable
\textcolor{red}{and elegant} pipelines.


\subsection{On-disk backend}

The main feature in version 2 of the \texttt{MSnbase} package was the
addition of different backends for raw data storage, namely
\textit{in-memory} and \textit{on-disk}. The following code chunk
demonstrates how to create two \texttt{MSnExp} objects \textcolor{red}{importing
  data from an mzML file and} storing \sout{data}\textcolor{red}{them}
either in-memory or on-disk.

<<readMSData, eval = FALSE, echo = TRUE>>=
library("MSnbase")
raw_mem <- readMSData("file.mzML", mode = "inMemory")
raw_dsk <- readMSData("file.mzML", mode = "onDisk")
@

<<time_sz>>=
load("bench_time_sz.rda")
time <- sapply(time_sz, function(x) x[, "time"])
sz <- sapply(time_sz, function(x) x[, "sz"])
colnames(sz) <- colnames(time) <- c(1, 5, 10)
rownames(sz) <- rownames(time) <- c("in-memory", "on-disk")
sz <- round(sz/(1014^2), 2)

p_read <-
    data.frame(time) %>%
    rownames_to_column() %>%
    pivot_longer(names_to = "files",
                 values_to = "time",
                 -rowname) %>%
    rename(backend = rowname) %>%
    mutate(files = as.numeric(sub("X", "", files))) %>%
    ggplot(aes(x = files, y = time, colour = backend)) +
    geom_point() +
    geom_line() +
    scale_x_continuous(breaks=c(1, 5, 10)) +
    ylab("Reading time [s]") +
    xlab("Number of files") +
    theme(legend.position = c(0.35, 0.86),
          legend.title = element_blank(),
          legend.text=element_text(size = 6))


p_sz <-
    data.frame(sz) %>%
    rownames_to_column() %>%
    pivot_longer(names_to = "files",
                 values_to = "size",
                 -rowname) %>%
    rename(backend = rowname) %>%
    mutate(files = as.numeric(sub("X", "", files))) %>%
    ggplot(aes(x = files, y = size, colour = backend)) +
    geom_point() +
    geom_line() +
    scale_x_continuous(breaks=c(1, 5, 10)) +
    ylab("Object size [MB]") +
    xlab("Number of files") +
    theme(legend.position = "none")
@

<<filt>>=
load("bench_t_filt.rda")
p_filt <-
    tibble(time = microbenchmark:::convert_to_unit(t_filt$time, "ms"),
           expr = as.character(t_filt$expr)) %>%
    mutate(mode = if_else(grepl("mem", expr), "in-memory", "on-disk")) %>%
    ggplot(aes(x = mode, y = time, fill = mode)) +
    ggplot2::geom_violin() +
    ggplot2::scale_y_log10() +
    ylab("Filtering time [ms]") +
    xlab("Backend") +
    theme(legend.position = "none")

@

<<access>>=
load("bench_t_access.rda")
p_access <-
    tibble(time = microbenchmark:::convert_to_unit(t_access$time, "ms"),
           expr = as.character(t_access$expr)) %>%
    mutate(n = sub("^access_.+_", "", expr)) %>%
    mutate(n = sub("all", 6103, n)) %>%
    mutate(mode = if_else(grepl("mem", expr), "in-memory", "on-disk")) %>%
    ggplot(aes(x = n, y = time, fill = mode)) +
    ggplot2::geom_violin() +
    ggplot2::scale_y_log10() +
    ylab("Raw data access [ms]") +
    xlab("Number of spectra (out of 6103)") +
    facet_wrap(~ mode) +
    theme(legend.position = "none")
@

\begin{figure}[p]
  \centering
<<plot_bench>>=
(p_read + p_sz + p_filt) / p_access +
    plot_layout(heights = c(1, 0.75)) +
    plot_annotation(tag_levels = 'a') &
    theme(axis.title = element_text(size = 10))
@
\caption{(a) Reading time (in seconds) and (b) data size in memory (in
  MB) to read/store 1, 5 and 10 files containing 1431 MS1 (on-disk
  only) and 6103 MS2 (on-disk and in-memory) spectra. (c) Filtering
  benchmark assessed over 10 interactions on in-memory and on-disk
  data containing 6103 MS2 spectra.  (d) Access time to spectra for
  the in-memory (left) and on-disk (right) backends for 1, 10, 100
  1000, 5000 and all 6103 spectra. Benchmarks were performed on a Dell
  XPS laptop with an Intel i5-8250U processor 1.60 GHz (4 cores, 8
  threads), 7.5 GB RAM running Ubuntu 18.04.4 LTS 64-bit
  \textcolor{red}{and an SSD drive}. }
\label{fig:bench}
\end{figure}

Both modes rely on the \texttt{mzR}~\cite{Chambers:2012} package to
access the spectra (using the \texttt{mzR::peaks()} function) and the
metadata (using the \texttt{mzR::header()} function) in the data
files. The former is the legacy storage mode, implemented in the first
version of the package, that loads all the raw data and the metadata
into memory upon creation of the in-memory \texttt{MSnExp}
object. This solution doesn't scale for modern large dataset, and was
complemented by the on-disk backend. The on-disk backend only loads
the metadata into memory when the on-disk \texttt{MSnExp} is created
and accesses the spectra data (i.e. m/z and intensity values) in the
original files on disk only when needed (see below and Figure
\ref{fig:bench} (d)), such as for example for plotting. There are two
direct benefits using the on-disk backend, namely faster reading and
reduced memory footprint. Figure \ref{fig:bench} shows 5-fold faster
reading times (a) and over a 10-fold reduction in memory usage (b).

\textcolor{red}{Because the on-disk backend does not hold all the data
  in memory, direct data manipulations are not possible. We thus
  implemented a \textit{lazy processing} mechanism for this backend
  that caches any data manipulation operations in a processing queue
  in the object itself. Only when the user accesses m/z or intensity
  values are these operations are applied}.  \sout{The on-disk backend
  also offers efficient data manipulation by way of \textit{lazy
    processing} in the object itself.} Operations on the raw data are
stored in a processing queue and only effectively applied when the
user accesses m/z or intensity values. \textcolor{red}{As an
  additional advantage, operations on subsets of the data become much
  faster since data manipulations are applied only to data subsets
  instead of the full data set at once. Also, data access in on-disk
  backends is parallelized by data file ensuring a higher performance
  of this backend over conventional in-memory data representations.}
As an example, the following short analysis pipeline, that can equally
be applied to in-memory or on-disk data, retains MS2 spectra acquired
between 1000 and 3000 seconds, extracts the m/z range corresponding to
the TMT 6-plex range and focuses on the MS2 spectra with a precursor
intensity greater than $11 \times 10^6$ (the median precursor
intensity).

<<filter, eval = FALSE, echo = TRUE>>=
ms <- ms %>%
    filterRt(c(1000, 3000)) %>%
    filterMz(120, 135)
ms[precursorIntensity(ms) > 11e6, ]
@

As shown on Figure~\ref{fig:bench} (c), this lazy mechanism is
significantly faster than its application on in-memory data. The
advantageous reading and execution times and memory footprint of the
on-disk backend are possible by retrieving only spectra data from the
selected subset hence avoiding access to the full raw data. Once
access to the spectra m/z and intensity values become mandatory (for
example for plotting), then the in-memory backend becomes more
efficient, as illustrated on Figure~\ref{fig:bench} (d).
\textcolor{red}{The benefit of accessing data in memory is however
  reduced by underlying copies that are performed during the
  subsetting operation. When subsetting an in-memory \texttt{MSnExp}
  into a new, smaller in-memory \texttt{MSnExp} instance, the matrices
  that contain the spectra for the new object are copied, thus leading
  to increased execution time and (transient, if the original data is
  replaced) memory usage. Figure~\ref{fig:bench} (d) shows that the
  larger the subset, that smaller the benefits of an in-memory
  backend. The example with the 6103 spectra, corresponding to the
  full data (i.e. all spectra are already in memory and there is no
  memory management overhead) is representative of memory access only
  and constitutes the best case scenario. } The gain of an in-memory
backend is maximal when the whole dataset is accessed \sout{(i.e. all
  spectra are already in memory)} and becomes negligible when large
subsets of the data are requested.


\subsection{Prototyping}

The \texttt{MSnExp} data structure and its interface constitute an
efficient prototyping environment for computational method
development. We illustrate this by demonstrating how to implement the
BoxCar\cite{Meier:2018} acquisition method. In a nutshell, BoxCar
acquisition aims at improving the detection of intact precursor ions
by distributing the charge capacity over multiple narrow m/z segments
and thus limiting the proportion of highly abundant precursors in each
segment. A full scan is reconstructed by combining the respective
adjacent segments of the BoxCar acquisitions. The
\texttt{MSnbaseBoxCar} package\cite{MSnbaseBoxCar} is a small package
that demonstrates this. The simple method is composed of three steps
is described below and illustrated with code from
\texttt{MSnbaseBoxCar} in the following code chunk.

\begin{enumerate}

\item Identify and filter the groups of spectra that represent
  adjacent BoxCar acquisitions (Figure~\ref{fig:bc}~(b)). This can be
  done using the \textit{filterString} metadata variable that
  identifies BoxCar spectra by their adjacent m/z segments with the
  \texttt{bc\_groups()} function and filtering relevant spectra with
  the \texttt{filterBoxCar()}.

\item Remove any signal outside the BoxCar segments using the
  \texttt{bc\_zero\_out\_box()} function from \texttt{MSnbaseBoxCar}
  (Figures~\ref{fig:bc}~(c) and (d)).


\item Using the \texttt{combineSpectra} function from the
  \texttt{MSnbase}, combine the cleaned BoxCar spectra into a new,
  full spectrum (Figure~\ref{fig:bc}~(e)).

\end{enumerate}

<<bc1, echo=TRUE, eval=FALSE>>=
bc <- readMSData("boxcar.mzML", mode = "onDisk") %>%
      bc_groups() %>%       ## identify BoxCar groups (creates 'bc_groups')
      filterBoxCar() %>%    ## keep only BoxCar spectra
      bc_zero_out_box() %>% ## remove signal outside of BoxCar segments
      combineSpectra(fcol = "bc_groups", ## reconstruct full spectrum
                     method = boxcarCombine)
@


After processing of the BoxCar data, the final object can either be
further analysed \sout{in R} \textcolor{red}{using \texttt{MSnbase}}
or written back to disk as an \texttt{mzML} file using
\texttt{writeMSData()} for processing with other tools.

\begin{figure}[p]
  \centering
<<plot_boxcar, message=FALSE, warning=FALSE, fig.height=8>>=
load("boxcar.rda")
p4 <- p4 + xlab("m/z")
p1 + p2 + p3 + p4 +
    plot_layout(ncol = 1,
                heights = c(0.7, 1.2, 0.7, 0.7)) +
    plot_annotation(tag_levels = 'a') &
    theme(axis.title = element_text(size = 10),
          axis.text = element_text(size = 6),
          plot.margin = margin(0, 0, 0, 0, "mm"))
@
\caption{BoxCar processing with \texttt{MSnbase}. (a) Standard full
  scan with (b) three corresponding BoxCar scans showing the adjacent
  segments. Figure (c) shows the overlapping intact BoxCar segments
  and (d) the same segments after cleaning, i.e. where peaks outside
  of the segments were removed. The reconstructed full scan is shown
  on panel (e). Spectra visualisation, as shown here, rely on the
  \texttt{ggplot2} \cite{ggplot2} package. }
\label{fig:bc}
\end{figure}

All the functions for the processing of BoxCar spectra and segments in
\texttt{MSnbaseBoxCar} were developed using existing functionality
implemented in \texttt{MSnbase}, illustrating the flexibility and
adaptability of the \texttt{MSnbase} package for computational mass
spectrometry method development.


\subsection{Visualisation}

The R environment is well known for the quality of its visualisation
capacity. This also holds true for mass
spectrometry\cite{Gatto:2015,protViz,Pviz,Bemis:2015}. Here, we
conclude the overview of version 2 of the \texttt{MSnbase} package by
highlighting the flexibility of the software to visualise and assess
the efficiency of raw data processing. Figure \ref{fig:cent} compares
the raw MS profile data \textcolor{red}{imported from an mzML file}
for \sout{the} serine and the same data after smoothing, centroiding
and m/z refinement, as illustrated in the code chunk below. Detailed
execution and description of these operations can be found in the
\emph{MSnbase: centroiding of profile-mode MS data} \texttt{MSnbase}
vignette.

<<serine_processing, echo=TRUE, eval=FALSE>>=
serine_mz <- 106.049871
serine_proc <- ms %>%
    smooth(method = "SavitzkyGolay", halfWindowSize = 4L) %>%
    pickPeaks(refineMz = "descendPeak") %>%
    filterMz(c(serine_mz - 0.01, serine_mz + 0.01)) %>%
    filterRt(c(175, 187))
@

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./figure/centroiding.pdf}
  \caption{Visualisation of data smoothing and m/z refinement using
    \texttt{MSnbase}. (a) Raw MS profile data for serine. Upper panel
    shows the base peak chromatogram (BPC), lower panel the individual
    signals in the retention time -- m/z space. The horizontal dashed
    red line indicates the theoretical m/z of the [M+H]+ adduct of
    serine. (b) Smoothed and centroided data for serine with m/z
    refinement. The horizontal red dashed line indicates the
    theoretical m/z for the [M+H]+ ion and the vertical red dotted
    line the position of the maximum signal. }
  \label{fig:cent}
\end{figure}


\subsection{\textcolor{red}{Package maintenance and governance}}

The first public commit to the \texttt{MSnbase} GitHub repository was
in October 2010. Since then, the package benefited from 12
contributors\cite{contribs} that added various features, some
particularly significant ones such as the on-disk backend described
herein. \textcolor{red}{Contributions to the package are explicitly
  encouraged, rewarded by an official contributor status and coverned
  by a code of conduct.}

According to \texttt{MSnbase}'s Bioconductor page, there are 36
Bioconductor packages that depend, import or suggest it. Among these
are \texttt{pRoloc}~\cite{Gatto:2014a} to analyse mass
spectrometry-based spatial proteomics data,
\texttt{msmsTests}~\cite{msmsTests}, \texttt{DEP}~\cite{Zhang:2018},
\texttt{DAPAR} and \texttt{ProStaR}~\cite{Wieczorek:2017} for the
statistical analysis \sout{for} \textcolor{red}{of} quantitative
proteomics data, \texttt{RMassBank}~\cite{Stravs:2013} to process
metabolomics tandem MS files and build MassBank records,
\texttt{MSstatsQC}~\cite{Dogu:2017} for longitudinal system
suitability monitoring and quality control of targeted proteomic
experiments and the widely used \texttt{xcms}~\cite{Smith:2006}
package for the processing and analysis of metabolomics
data. \texttt{MSnbase} is also used in non-R/Bioconductor software,
such as for example IsoProt~\cite{Griss:2019}, that provides a
reproducible workflow for iTRAQ/TMT experiments. \textcolor{red}{The
  BioContainers~\cite{da_Veiga_Leprevost:2017} project offers a
  dedicated container for the \texttt{MSnbase} package, this
  facilitating the reuse of the package in third-party pipelines.}
\texttt{MSnbase} currently ranks 101 out of 1823 packages based on the
monthly downloads from unique IP addresses, tallying over 1000
downloads from unique IP addresses each months.

As is custom with Bioconductor packages, \texttt{MSnbase} comes with
ample documentation. Every user-accessible function is documented in a
dedicated manual page. In addition, the package offers 5 vignettes,
including one aimed at developers. The package is checked nightly on
the Bioconductor servers: it implements unit tests covering 72\% of
the code base and, through its vignettes, also provides integration
testing. Questions from users and developers are answered on the
Bioconductor support forum as well as on the package GitHub page. The
package provides several sample and benchmarking datasets, and relies
on other dedicated \textit{experiment packages} such as
\texttt{msdata}~\cite{msdata} for raw data or
\texttt{pRolocdata}~\cite{Gatto:2014a} for quantitative
data. \texttt{MSnbase} is available on Windows, Mac OS and Linux under
the open source Artistic 2.0 license and easily installable using
standard installation procedures.


\textcolor{red}{The growth of \texttt{MSnbase} and the user support
  provided over the years attest to the core maintainers commitment to
  long-term development, and the quality and maintainability of the
  code base.}

\section{Discussion}

We have presented here some important functionality of
\texttt{MSnbase} version 2. The new on-disk infrastructure enables
large scale data analyses~\cite{Nothias:2020}, either using
\texttt{MSnbase} directly or through packages that rely on it, such as
\texttt{xcms}. We have also illustrated how \texttt{MSnbase} can be
used for standard data analysis and visualisation, and how it can be
used for method development and prototyping.



<<version>>=
v <- packageVersion("MSnbase")
@

The version of \texttt{MSnbase} used in this manuscript is
\Sexpr{v}. The main features presented here were available since
version 2.0. The code to reproduce the analyses and figures in this
article is available at
\url{https://github.com/lgatto/2020-msnbase-v2/}.


\begin{acknowledgement}

The authors thank the various contributors and users who have provided
constructive input and feedback that have helped, over the years, the
improvement of the package. The authors declare no conflict of
interest.

\end{acknowledgement}

%% \begin{suppinfo} %% Supporting Information
%%
%% A listing of the contents of each file supplied as Supporting Information
%% should be included. For instructions on what should be included in the
%% Supporting Information as well as how to prepare this material for
%% publications, refer to the journal's Instructions for Authors.
%%
%% The following files are available free of charge.
%% \begin{itemize}
%%   \item Filename: brief description
%%   \item Filename: brief description
%% \end{itemize}
%% \end{suppinfo}


\bibliography{refs}

\end{document}
